{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"poUufOUKExXR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from textblob import TextBlob\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1mbWyGxE9Qq"},"outputs":[],"source":["from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bs0kj_SfE_nP"},"outputs":[],"source":["pip install python-dotenv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwtP9abrFCtZ"},"outputs":[],"source":["# Initial imports\n","import os\n","import pandas as pd\n","from datetime import datetime, timedelta\n","from dotenv import load_dotenv\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NsgsMuoRFC3u"},"outputs":[],"source":["df = pd.read_csv('223k crypto news headlines. Dataset. BDCenter Digital.csv')\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwmgUtEoYe1z"},"outputs":[],"source":["def getPolarity(newstext):\n","  return TextBlob(newstext).sentiment.polarity\n","\n","df['Polarity'] = df['headlinetext'].apply(getPolarity)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0P0lC-MmFC6E"},"outputs":[],"source":["df.isnull().any()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVfBuLX3FOn4"},"outputs":[],"source":["pip install --user scipy wordcloud nltk seaborn textblob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aa8TwCvgFOqt"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import json, nltk\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import seaborn as sns\n","# nltk.download('wordnet')   # for Lemmatization\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzPg2ndoFOtY"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x48i6DSacSxn"},"outputs":[],"source":["#df['Polarity'] = pd.to_numeric(df['Polarity'],downcast='integer')\n","df['Polarity'] = df['Polarity'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sm7OqeSlfxYN"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG08KM0feCJp"},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgNlHEQ0FOwN"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWsFFGgzFOzm"},"outputs":[],"source":["import re\n","\n","def process_text(text):\n","    text = text.lower()                                             # Lowercases the string\n","    text = re.sub('@[^\\s]+', '', text)                              # Removes usernames\n","    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text)   # Remove URLs\n","    text = re.sub(r\"\\d+\", \" \", str(text))                           # Removes all digits\n","    text = re.sub('\u0026quot;',\" \", text)                               # Remove (\u0026quot;) \n","                                               # Replaces Emojis\n","    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(text))                   # Removes all single characters\n","    for word in text.split():\n","        #if word.lower() in contractions:\n","            #text = text.replace(word, contractions[word.lower()])   # Replaces contractions\n","            text = re.sub(r\"[^\\w\\s]\", \" \", str(text))                       # Removes all punctuations\n","            text = re.sub(r'(.)\\1+', r'\\1\\1', text)                         # Convert more than 2 letter repetitions to 2 letter\n","            text = re.sub(r\"\\s+\", \" \", str(text))                           # Replaces double spaces with single space    \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h75G6azBFC9N"},"outputs":[],"source":["#df['processed_text'] = np.vectorize(process_text)(df[text])\n","df['processed_text'] = np.vectorize(process_text)(df['headlinetext'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J75RvNDSFC_o"},"outputs":[],"source":["df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkgxtYtyFDDO"},"outputs":[],"source":["#Tokenization\n","tokenized_text = df['processed_text'].apply(lambda x: x.split())\n","tokenized_text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfdkvNX3KyyS"},"outputs":[],"source":["#STEMMING\n","\n","#from nltk.stem.porter import *\n","#stemmer = PorterStemmer()\n","\n","#tokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x])\n","#tokenized_text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3Pv-JqVLz3G"},"outputs":[],"source":["import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rn1ngXzALT80"},"outputs":[],"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","tokenized_text = tokenized_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n","tokenized_text.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3Lhepb6Qwjv"},"outputs":[],"source":["#nltk.download(\"stopwords\")\n","#from nltk.corpus import stopwords\n","#stop_words = set(stopwords.words('english'))\n","#stop_words, nltk.corpus.stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0epPDfpLL54T"},"outputs":[],"source":["#for i in range(len(tokenized_text)):\n","    \n","     # Below code is used for no stop word removal\n","     #tokenized_text[i] = ' '.join(tokenized_text[i]) \n","    \n","     # Used for stop word removal \n","     # (Below is commented out as sentiment analysis is giving better accuracy without removing stop words.\n","     # If you still want to check, comment out the above line, uncomment the line below and run the code again.)\n","    \n","     #tokenized_text[i] = ' '.join([word for word in tokenized_text[i] if word not in stop_words])  \n","    \n","\n","#df['processed_text'] = tokenized_text\n","#df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLw-YvZPRDlR"},"outputs":[],"source":["#Feature extraction (vectorization)\n","#1. Count Vectorizer\n","#2. Tf-Idf vectorizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztuH5TEERW-j"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","count_vectorizer = CountVectorizer(ngram_range=(1,2))    # Unigram and Bigram\n","final_vectorized_data = count_vectorizer.fit_transform(df['processed_text'])  \n","final_vectorized_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwreJnLFlLI2"},"outputs":[],"source":["#from sklearn.feature_extraction.text import TfidfVectorizer \n","\n","#tf_idf_vectorizer = TfidfVectorizer(use_idf=True,ngram_range=(1,3))\n","#final_vectorized_data = tf_idf_vectorizer.fit_transform(df['processed_text'])\n","\n","#final_vectorized_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0s6RQzmYqiAu"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, df['Polarity'],\n","                                                    test_size=0.2, random_state=69)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIaLR0WbqiEQ"},"outputs":[],"source":["print(\"X_train_shape : \",X_train.shape)\n","print(\"X_test_shape : \",X_test.shape)\n","print(\"y_train_shape : \",y_train.shape)\n","print(\"y_test_shape : \",y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JplxzRxQqiHs"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_K5oKFBco3ii"},"outputs":[],"source":["#standardization\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler(with_mean=False)\n","\n","X_train = sc.fit_transform(X_train)\n","\n","X_test = sc.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MYxxxr8xrAj9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Accuracy : 1.0\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","print(\"Training Accuracy :\", model.score(X_train, y_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaAFQYDdrweX"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test,y_pred))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOdbTFL7BNONCFXSlAJQoRq","collapsed_sections":[],"name":"SentimentAnalysis_probabilistic.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}